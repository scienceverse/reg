---
title: "Using scienceverse for Registered Reports"
author: "Daniel Lakens & Lisa DeBruine"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

#devtools::install_github("scienceverse/scienceverse")
#install.packages(TOSTER)

library(scienceverse)
library(TOSTER)
options(scipen=99)

```


The goal of scienceverse is to generate and process machine-readable study descriptions. Studies are described as JSON files on the levels of the hypothesis, methods, data, and analysis. These machine readable description can be used in several ways, such as:

1. Generate a pre-registration file that specifies how each hypothesis will be analyzed and evaluated.
2. Generate a post-registration file that evaluates, based on the data, whether running preregistered analyses support the predictions.
3. Search archival JSON files for variables, measures, and data.
4. Automated reporting of statistical tests.
5. Reproduce the reported results by running the analysis code on the data.

In this working vignette we demonstrate points 1 and 2 above. 

## Installation

You can install the released version of `scienceverse` from [GitHub](https://github.com/scienceverse/scienceverse) with:

``` r
devtools::install_github("scienceverse/scienceverse")
```

## A Study to Distinguish Apathy from Depression

We plan to perform a study that tests whether apathy is distinct from depression. Apathy is defined as diminished motivation, while depression should involve emotional distress. Where earlier theoretical work has suggested apathy is part of depression, our theoretical model suggests the two should be distinct. We measure peoples apathy score using the Apathy Scale, and depression using the Depression Scale. Although we do not assume the correlation between the two measurements is exactly zero, we predict the two measurements will show a correlation that is smaller than 0.3. If so, we will take this finding as support for our prediction that apathy and depression are distinct enough, such that apathy should not be considered a part of depression.

To set up the study, it makes sense to first think about what our data will look like, and then what our statistical hypothesis is. We will collect data from two scales. We know these scales have 5 items each, we will analyze the average score for each of the two scales. We will name these columns 'apathy' and 'depression', and calculate them from the mean of the five apathy items (a1 to a5 in our dataframe) and the five depression items (d1 to d5 in our dataframe). 

Our statistical hypothesis is that we will interpret the data as support for our prediction when we can statistically reject effects larger than *r* = 0.3. We can do this by performing an equivalence test, and checking whether the observed correlation is statistically smaller than 0.3.

We can enter all information we need to specify our hypothesis below.

### Setting up the study

We set up our study by giving it a name using the `study` function. We can also add any custom information after the name, such as authors or an abstract.

```{r}
ap_dep_study <- study("Distinguishing Apathy from Depression",
                      author = c("Daniel Lakens", "Lisa DeBruine"))
```

The object we created is basically a list of lists, which we will populate with information:

```{r}
str(ap_dep_study)
```

One goal of scienceverse is to automate the evaluation of predictions. A researcher specifies the prediction in the preregistration, collects the data, and scienceverse can then take the preregistration file and the data and automatically evaluate whether the predictions were confirmed or not.

### Add the hypotheses

First, we need to add each hypothesis using the `add_hypothesis()` function. We can describe our hypothesis as "The correlation between the apathy and depression scale is smaller than 0.3." We also add an ID to the hypothesis so we can link it to the corresponding analyses later on - naming them H1, H2, H3a etc is typically enough.

```{r}
ap_dep_study <- add_hypothesis(
  ap_dep_study, 
  "The correlation between the apathy and depression scale is smaller than 0.3", 
  id = "H1"
)
```

### Add data prep

Often, the raw data needs to be prepared for analysis. For example, outliers might need to be removed, scales need to be summed, and other calculations are performed. In general, it is recommended to never change the raw data files. Instead, you read in the raw data files, and save an analysis data file where all calculations are performed, outliers are removed, etc. So we run the data preparation code from a dedicated R script that pre-processes the raw data on the code. 

You can provide code to do this using the `add_prep()` function. You can write your code in a separate .R file and read it into the function by setting the `code` argument to the file path. You can provide it with any attached data files by setting them in the list of `params`. In this example, the script expects there to be a data frame named `raw_data` and this will be loaded from the attached data with the ID `apathy_depression_raw`. You can specify an attached data frame as ".data[data_id]" (attaching data will be covered below). The `return` argument is a list of object names to return from the script. Our example creates a cleaned data fame called `processed_data`, which we will use in our analysis code below.

```{r}
ap_dep_study <- add_prep(
  ap_dep_study, 
  code = "command_files/data_prep.R",
  params = list("raw_data" = ".data[apathy_depression_raw]"),
  return = c("processed_data")
)
```

### Add the analyses

Another goal of scienceverse is to remove ambiguity in how hypotheses are specified. The best way to preregister a hypothesis is to write the analysis script before the data is collected. Scienceverse takes this analysis script and combines it with user-defined evaluation criteria. These make it clear when a hypothesis is confirmed in the preregistration, and can also generate an automatic evaluation of the hypotheses.

It means we need to have an analysis script for our data. It also means it is typically recommended to simulate data with the same structure as the data you will collect. You can then run your analysis code on the simulated data, and see if you can perform the tests you want to run on the real data. In this example we plan to perform an equivalence test. We have written a function, named `eq_test_r.R` that takes in a dataframe and performs an equivalence test. The script is:

```{r}
source("command_files/eq_test_r.R")
eq_test_r
```

The test we will run on the data requires certain parameters that specify the input, such as data (which dataframe will be used), which columns in the dataframe are used, what is the alpha level, etc. The function also gives output (`test_res`) which we can use to evaluate the test results (e.g., the *p*-value for the equivalence test). For now, the output must be a named list.

Below we specify the study we want to add an analysis to (`ap_dep_study`), and specify the function that should be run (`"eq_test_r"`). Then we specify *all* the parameters that our function needs to run. These are basically the values you need to specify for any R function to run without throwing an error for a missing argument. We also add an analysis ID - you might have many different analyses you will run on a data file, and distinguishing them based on what they do (e.g., manipulation_check, main_analysis) might be useful.

```{r}
ap_dep_study <- add_analysis(
  ap_dep_study,
  func = "eq_test_r",
  params = list(
    data = ".data[processed_data]",
    col1 = "apathy",
    col2 = "depression",
    alpha = 0.05,
    high_eqbound_r = 0.3,
    low_eqbound_r = -0.3
  ),
  id = "main_analysis"
)
```

### Add criteria for each hypothesis

Now that we have specified our analysis, we need to be explicit about the values that we will use to evaluate the hypotheses. We can use any values in the list of results from `test_res` to evaluate the result. In this case, we will perform an equivalence test. To specify the statistical conditions that need to be met, we look at the `TOSTr()` function. The output value we need for our hypothesis test is called `TOST_p2`, which is the p-value against the upper bound. So when we set the upper equivalence bound to 0.3, our prediction is supported if `TOST_p2` is smaller than our alpha level.

We plan to collect a large sample size of 460 people, and should have very high power for the equivalence test, and to balance our error rates, we set the alpha level to 0.01. Because we will compare our p-value to the alpha level, our `comparator` is 0.01, and our hypothesis is supported when the p-value is smaller than 0.01, and therefore we specify the `operator` as `<`. Note that this example uses Hypothesis Testing, but you can also make other predictions, such as a mean that is larger than some value, or any other prediction based on parameters from the analyses you perform.

To evaluate our hypotheses, we add criteria to our study using the `add_criterion()` function. Our hypothesis `H1` will be considered confirmed when the result of the `main_analysis` has `TOST_p2` smaller than our alpha level of 0.01.  We link this criterion to the hypothesis and analysis that it relates to using their IDs (`H1` and `main_analysis`).

```{r}
ap_dep_study <- add_criterion(
  ap_dep_study, 
  result = "TOST_p2", 
  operator = "<", 
  comparator = 0.01,
  analysis_id = "main_analysis",
  hypothesis_id = "H1"
)
```

You can add more than one criterion to a hypothesis by calling the `add_criterion()` function more than once. By default, hypotheses are evaluated as true if all of their criteria are true. However, you can set up a hypothesis to evaluate to true if at least one criterion is true by adding the argument `evaluation = "|"` when you set up the hypothesis using `add_hypothesis()`.

### Save the study archive file

This `study` list of lists can be stored as an archive file. This saves the information in plain text JSON format.

```{r, eval = FALSE}
# save the framework to a JSON file
study_save(ap_dep_study, "pre_data_apathy_depression.json")
```

We can read back the JSON file into an R list and take a look at the structure. We first remove the files from R:

```{r}
# remove study and function to load from JSON
rm(ap_dep_study)
rm(eq_test_r)
```

And then read them back in:

```{r}
ap_dep_study <- study("pre_data_apathy_depression.json")
str(ap_dep_study)
```

## Pre-registration

Because we specified our test and evaluation criteria for our prediction in detail in the JSON file, we can automatically extract this information, and summarize it in a human-readable format that can be used to preregister our statistical prediction with enough detail so that there is no ambiguity in our prediction, or what would support our prediction. 

We can do this by creating a summary of the JSON file that contains the sections that are relevant for the preregistration. In this case, it means running `study_report` command, asking for the 'prereg' template. This will write an html file named `prereg_ap_dep_study.html`.


```{r, results='asis'}
study_report(ap_dep_study, 
             template = "prereg", 
             filename = "prereg_ap_dep_study.html")

```

This writes an .html file to the working directory, that should look like:

----------------------------------------------------------------

<h1>Registration of Statistical Hypotheses</h1>
<h3>21 June, 2019</h3>
<h2>Distinguishing Apathy from Depression Preregistration</h2>
<h2>Hypotheses</h2>
<h3>Hypothesis 1</h3>

The correlation between the apathy and depression scale is smaller than 0.3

    Criterion 1 is confirmed if analysis yields TOST_p2 < 0.01

If all criteria are met, this hypothesis is supported.

<h2>Analyses</h2>

We will run `eq_test_r(data = .data[1], col1 = apathy, col2 = depression, alpha = 0.05, high_eqbound_r = 0.3, low_eqbound_r = -0.3)`

----------------------------------------------------------------

## Post-registration

After the preregistration we collect the data. Our data has 5 columns for the apathy items (a1 to a5) and 5 columns for the depression data (d1 to d5). 

### Attach the data

Now that we have 'collected' (in this case simulated) the data we can use the scienceverse package to evaluate the preregistered results. The scienceverse package does this by taking the data, running the preregistered analysis script, and comparing the results to the preregistered evaluation criteria. We will first load the data, which we assume is stored in a folder named 'original_data'. We give the data an ID so we can reference it later (in case there are multiple hypotheses, based on different raw data files).

```{r}
ap_dep_study <- add_data(
  ap_dep_study, 
  "original_data/apathy_depression_raw.csv", 
  id = "apathy_depression_raw"
)
```

### Data prep

We first run the data preparation on the raw data (this is a seperate step, for cases in which the data preparation takes a long time, so it only needs to be performed once).

```{r, results='asis'}
ap_dep_study <- data_prep(ap_dep_study)
```

This will create a new data entry in `ap_dep_study` called `processed_data` (the name we specified in the `data_prep()` function).

### Run the analysis

We preregistered that we would consider the results supported when the p-value for the test against the upper equivalence bound (a correlation of r = 0.3) would be smaller than the alpha level of 0.01. Now that our data are attached and prepped, we can evaluate the pre-registered hypotheses automatically. The summary returns a conclusion, based on the planned analysis and the collected data. 

```{r, results='asis'}
ap_dep_study <- study_analyze(ap_dep_study)
```

The evaluation of our analysis is performed automatically. It demonstrates how machine readable hypotheses are an easy way to check whether the predictions that were made in a preregistration are formally supported. We can generate a report that can be submitted with the manuscript that summarizes whether each pre-registered hypothesis was supported:

```{r}
study_report(ap_dep_study, 
             template = "postreg", 
             filename = "postreg_ap_dep_study.html")
```

Finally, when we are all done, we save the archive file. This contains everything we need to reproduce the reported results: the raw data, the data preparation, the analyses, and the results. Pretty cool, right? ;)

```{r}
study_save(ap_dep_study, "post_study.json")
```
